{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Retainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has been taken from Kaggle. It contains 14999 rows and 10 columns. These features help measure the Employee satisfaction in a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_Data = pd.read_csv('HR_comma_sep.csv')\n",
    "Emp_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting categorical columns to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['department','salary']\n",
    "emp_data_final = pd.get_dummies(Emp_Data,columns=feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emp_data_final.drop(['left'],axis=1).values\n",
    "y = emp_data_final['left'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data to scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = emp_data_final.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n",
    "\n",
    "plt.title('Correlation between different fearures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satisfaction_by_dept=Emp_Data.groupby('department').mean()\n",
    "satisfaction_by_dept.sort_values(by=\"satisfaction_level\", ascending=True, inplace=True)\n",
    "satisfaction_by_dept\n",
    "y_pos = np.arange(len(satisfaction_by_dept.index))\n",
    "\n",
    "plt.barh(y_pos, satisfaction_by_dept['satisfaction_level'], align='center', alpha=0.8)\n",
    "plt.yticks(y_pos, satisfaction_by_dept.index)\n",
    "\n",
    "plt.xlabel('Satisfaction level')\n",
    "plt.title('Mean Satisfaction Level of each department')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting is an ensemble ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HARD VOTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority/ mode based voting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "#Hard Voting - We build our models with decision tree, support vector machines and logistic regression algorithms\n",
    "# Let's create the sub models\n",
    "estimators = []\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "estimators.append(('DecisionTree', dt_model))\n",
    "\n",
    "svm_model = SVC(random_state=1)\n",
    "estimators.append(('SupportVector', svm_model))\n",
    "\n",
    "logit_model = LogisticRegression(random_state=1)\n",
    "estimators.append(('Logistic Regression', logit_model))\n",
    "\n",
    "#We build individual models with each of the classifiers we have chosen\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for each_estimator in (dt_model, svm_model, logit_model):\n",
    "    each_estimator.fit(X_train, Y_train)\n",
    "    Y_pred = each_estimator.predict(X_test)\n",
    "    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))\n",
    "    \n",
    "#We proceed to ensemble our models and use VotingClassifier to score accuracy\n",
    "# Using VotingClassifier() to build ensemble model with Hard Voting\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='hard')\n",
    "ensemble_model.fit(X_train,Y_train)\n",
    "predicted_labels = ensemble_model.predict(X_test)            \n",
    "print(\"Classifier Accuracy using Hard Voting: \", accuracy_score(Y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.SOFT VOTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the prediction of class based on the average of probability given to that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft Voting - The below code creates an ensemble using soft voting:\n",
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "estimators.append(('DecisionTree', dt_model))\n",
    "\n",
    "svm_model = SVC(random_state=1, probability=True)\n",
    "estimators.append(('SupportVector', svm_model))\n",
    "\n",
    "logit_model = LogisticRegression(random_state=1)\n",
    "estimators.append(('Logistic Regression', logit_model))\n",
    "\n",
    "for each_estimator in (dt_model, svm_model, logit_model):\n",
    "    each_estimator.fit(X_train, Y_train)\n",
    "    Y_pred = each_estimator.predict(X_test)\n",
    "    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))\n",
    "# Using VotingClassifier() to build ensemble model with Soft Voting\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "ensemble_model.fit(X_train,Y_train)\n",
    "predicted_labels = ensemble_model.predict(X_test)            \n",
    "print(\"Classifier Accuracy using Soft Voting: \", accuracy_score(Y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_1 = RandomForestClassifier(random_state=0, n_estimators=10)\n",
    "rf_1.fit(X_train, Y_train)\n",
    "\n",
    "rf_2 = RandomForestClassifier(random_state=0, n_estimators=50)\n",
    "rf_2.fit(X_train, Y_train)\n",
    "\n",
    "rf_3 = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "rf_3.fit(X_train, Y_train)\n",
    "\n",
    "# combine all three Voting Ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = [('rf_1', rf_1), ('rf_2', rf_2), ('rf_3', rf_3)]\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "ensemble.fit(X_train, Y_train)\n",
    "print(\"rf_1.score: \", rf_1.score(X_test, Y_test))\n",
    "print(\"rf_2.score: \", rf_2.score(X_test, Y_test))\n",
    "print(\"rf_3.score: \", rf_3.score(X_test, Y_test))\n",
    "print(\"ensemble.score based on hard voting: \", ensemble.score(X_test, Y_test))\n",
    "\n",
    "estimators = [('rf_1', rf_1), ('rf_2', rf_2), ('rf_3', rf_3)]\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_train, Y_train)\n",
    "print(\"rf_1.score: \", rf_1.score(X_test, Y_test))\n",
    "print(\"rf_2.score: \", rf_2.score(X_test, Y_test))\n",
    "print(\"rf_3.score: \", rf_3.score(X_test, Y_test))\n",
    "print(\"ensemble.score based on soft voting: \", ensemble.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(base_estimator=SVC(),\n",
    "n_estimators=10, random_state=0).fit(X_train, Y_train)\n",
    "print(bag.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "#base learners\n",
    "estimators = [\n",
    "              (\"rf\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "              (\"svr\", make_pipeline(StandardScaler(), LinearSVC(max_iter=10000,random_state=42))),\n",
    "              ]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "clf.fit(X_train, Y_train).score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100).fit(X_train,Y_train)\n",
    "scores = cross_val_score(clf, X_test, Y_test, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train,Y_train)\n",
    "scores = cross_val_score(clf, X_test, Y_test, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Initializing Classifiers\n",
    "#clf is the classifier object being returned from sklearn.\n",
    "clf1 = LogisticRegression(random_state=0)\n",
    "clf2 = RandomForestClassifier(random_state=0)\n",
    "clf3 = SVC(random_state=0, probability=True)\n",
    "eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],weights=[2, 1, 1], voting='soft')\n",
    "\n",
    "# Plotting Decision Regions\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "X_plot = X[:,[0, 2]]\n",
    "labels = ['Logistic Regression',\n",
    "          'Random Forest',\n",
    "          'RBF kernel SVM',\n",
    "          'Ensemble']\n",
    "\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1],\n",
    "                         repeat=2)):\n",
    "    clf.fit(X_plot, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X_plot, y=y,\n",
    "                                clf=clf, legend=1)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_pca, Y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_pca)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train_pca, Y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test_pca, Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "X_test_kpca = kpca.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_kpca, Y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_kpca)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train_kpca, Y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test_kpca, Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train, Y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_lda, Y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test_lda)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, y_pred)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train_lda, Y_train\n",
    "X1= np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01))\n",
    "                     \n",
    "plt.contourf(X1,classifier.predict(np.array([X1.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
